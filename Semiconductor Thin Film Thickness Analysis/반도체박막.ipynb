{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JTYB95XYcOPs"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sklearn\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import torch\n",
    "#import torchvision\n",
    "#import torchtext\n",
    "from sklearn.preprocessing import LabelEncoder # 라벨 인코더\n",
    "\n",
    "import os\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"\"  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "colab_type": "code",
    "id": "AmB97wEIvcs8",
    "outputId": "896be15b-c195-4d49-baf1-aceb686f9d5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1110771377565522077\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4937233203\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16404981073241555991\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow as tf\n",
    "print(tf.test.is_gpu_available())\n",
    "print(device_lib.list_local_devices())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "kmERhoBE48Zk",
    "outputId": "be2db5f0-d2f4-4bd1-81f8-2e6a3dbd45e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "65uW1T2s6EmW",
    "outputId": "764705ad-7663-4fba-b4de-e254d1aeb9f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\na = tf.constant(1)\\n\\nb = tf.constant(2)\\n\\nsess = tf.Session()\\n\\nc = a + b\\n\\nprint('Sum = ', sess.run(c))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\"\"\"\n",
    "a = tf.constant(1)\n",
    "\n",
    "b = tf.constant(2)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "c = a + b\n",
    "\n",
    "print('Sum = ', sess.run(c))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-Ag7OL7pCA7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "#K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dNtNIUQ6UDz"
   },
   "outputs": [],
   "source": [
    "#데이터 위치 train,test, sample_submission\n",
    "#https://www.dacon.io/competitions/official/235554/overview/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "m_IzGLF8cQbG",
    "outputId": "5cde9479-47df-491d-b076-7e64548973fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>layer_1</th>\n",
       "      <th>layer_2</th>\n",
       "      <th>layer_3</th>\n",
       "      <th>layer_4</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.254551</td>\n",
       "      <td>0.258823</td>\n",
       "      <td>0.254659</td>\n",
       "      <td>0.252085</td>\n",
       "      <td>0.247678</td>\n",
       "      <td>0.253614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354750</td>\n",
       "      <td>0.369223</td>\n",
       "      <td>0.388184</td>\n",
       "      <td>0.408496</td>\n",
       "      <td>0.414564</td>\n",
       "      <td>0.429403</td>\n",
       "      <td>0.419225</td>\n",
       "      <td>0.443250</td>\n",
       "      <td>0.433414</td>\n",
       "      <td>0.465502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>0.205062</td>\n",
       "      <td>0.225544</td>\n",
       "      <td>0.217758</td>\n",
       "      <td>0.202169</td>\n",
       "      <td>0.199633</td>\n",
       "      <td>0.207380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557203</td>\n",
       "      <td>0.573656</td>\n",
       "      <td>0.587998</td>\n",
       "      <td>0.612754</td>\n",
       "      <td>0.627825</td>\n",
       "      <td>0.633393</td>\n",
       "      <td>0.637706</td>\n",
       "      <td>0.625981</td>\n",
       "      <td>0.653231</td>\n",
       "      <td>0.637853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>30</td>\n",
       "      <td>0.189196</td>\n",
       "      <td>0.165869</td>\n",
       "      <td>0.177655</td>\n",
       "      <td>0.156822</td>\n",
       "      <td>0.175094</td>\n",
       "      <td>0.177755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699864</td>\n",
       "      <td>0.708688</td>\n",
       "      <td>0.721982</td>\n",
       "      <td>0.713464</td>\n",
       "      <td>0.743030</td>\n",
       "      <td>0.741709</td>\n",
       "      <td>0.747743</td>\n",
       "      <td>0.746037</td>\n",
       "      <td>0.737356</td>\n",
       "      <td>0.750391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.120076</td>\n",
       "      <td>0.138975</td>\n",
       "      <td>0.117931</td>\n",
       "      <td>0.130566</td>\n",
       "      <td>0.131262</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764786</td>\n",
       "      <td>0.763788</td>\n",
       "      <td>0.770017</td>\n",
       "      <td>0.787571</td>\n",
       "      <td>0.778866</td>\n",
       "      <td>0.776969</td>\n",
       "      <td>0.774712</td>\n",
       "      <td>0.801526</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.784057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.091033</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>0.108125</td>\n",
       "      <td>0.080405</td>\n",
       "      <td>0.105917</td>\n",
       "      <td>0.077083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786677</td>\n",
       "      <td>0.802271</td>\n",
       "      <td>0.806557</td>\n",
       "      <td>0.799614</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>0.804087</td>\n",
       "      <td>0.787763</td>\n",
       "      <td>0.794948</td>\n",
       "      <td>0.819105</td>\n",
       "      <td>0.801781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   layer_1  layer_2  layer_3  layer_4         0         1         2         3  \\\n",
       "0       10       10       10       10  0.254551  0.258823  0.254659  0.252085   \n",
       "1       10       10       10       20  0.205062  0.225544  0.217758  0.202169   \n",
       "2       10       10       10       30  0.189196  0.165869  0.177655  0.156822   \n",
       "3       10       10       10       40  0.131003  0.120076  0.138975  0.117931   \n",
       "4       10       10       10       50  0.091033  0.086893  0.108125  0.080405   \n",
       "\n",
       "          4         5  ...       216       217       218       219       220  \\\n",
       "0  0.247678  0.253614  ...  0.354750  0.369223  0.388184  0.408496  0.414564   \n",
       "1  0.199633  0.207380  ...  0.557203  0.573656  0.587998  0.612754  0.627825   \n",
       "2  0.175094  0.177755  ...  0.699864  0.708688  0.721982  0.713464  0.743030   \n",
       "3  0.130566  0.131262  ...  0.764786  0.763788  0.770017  0.787571  0.778866   \n",
       "4  0.105917  0.077083  ...  0.786677  0.802271  0.806557  0.799614  0.789333   \n",
       "\n",
       "        221       222       223       224       225  \n",
       "0  0.429403  0.419225  0.443250  0.433414  0.465502  \n",
       "1  0.633393  0.637706  0.625981  0.653231  0.637853  \n",
       "2  0.741709  0.747743  0.746037  0.737356  0.750391  \n",
       "3  0.776969  0.774712  0.801526  0.805305  0.784057  \n",
       "4  0.804087  0.787763  0.794948  0.819105  0.801781  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv('\\data/train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iPoTyoXR5C7q"
   },
   "outputs": [],
   "source": [
    "np.shape(train)\n",
    "train=train.sample(frac=1) #shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "FJpsKmlOci_P",
    "outputId": "018ca418-f4ce-4c50-c355-328b4e93ead2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.535410</td>\n",
       "      <td>0.520775</td>\n",
       "      <td>0.494087</td>\n",
       "      <td>0.465134</td>\n",
       "      <td>0.430339</td>\n",
       "      <td>0.401751</td>\n",
       "      <td>0.355986</td>\n",
       "      <td>0.326427</td>\n",
       "      <td>0.282340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748339</td>\n",
       "      <td>0.757575</td>\n",
       "      <td>0.768130</td>\n",
       "      <td>0.777062</td>\n",
       "      <td>0.769173</td>\n",
       "      <td>0.768253</td>\n",
       "      <td>0.738704</td>\n",
       "      <td>0.739460</td>\n",
       "      <td>0.702139</td>\n",
       "      <td>0.702238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.351099</td>\n",
       "      <td>0.398179</td>\n",
       "      <td>0.413809</td>\n",
       "      <td>0.418529</td>\n",
       "      <td>0.433257</td>\n",
       "      <td>0.455410</td>\n",
       "      <td>0.451065</td>\n",
       "      <td>0.464230</td>\n",
       "      <td>0.476011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333931</td>\n",
       "      <td>0.276307</td>\n",
       "      <td>0.211513</td>\n",
       "      <td>0.159223</td>\n",
       "      <td>0.110982</td>\n",
       "      <td>0.083130</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.145420</td>\n",
       "      <td>0.260501</td>\n",
       "      <td>0.343857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.490537</td>\n",
       "      <td>0.435958</td>\n",
       "      <td>0.413428</td>\n",
       "      <td>0.355796</td>\n",
       "      <td>0.335777</td>\n",
       "      <td>0.299944</td>\n",
       "      <td>0.242745</td>\n",
       "      <td>0.210555</td>\n",
       "      <td>0.180739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709371</td>\n",
       "      <td>0.746826</td>\n",
       "      <td>0.781436</td>\n",
       "      <td>0.788292</td>\n",
       "      <td>0.828630</td>\n",
       "      <td>0.835166</td>\n",
       "      <td>0.845859</td>\n",
       "      <td>0.846032</td>\n",
       "      <td>0.836724</td>\n",
       "      <td>0.846779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.051634</td>\n",
       "      <td>0.075802</td>\n",
       "      <td>0.133983</td>\n",
       "      <td>0.154546</td>\n",
       "      <td>0.209387</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.287552</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.340617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075046</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>0.079884</td>\n",
       "      <td>0.147469</td>\n",
       "      <td>0.213112</td>\n",
       "      <td>0.298096</td>\n",
       "      <td>0.382823</td>\n",
       "      <td>0.489381</td>\n",
       "      <td>0.562383</td>\n",
       "      <td>0.599247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.154031</td>\n",
       "      <td>0.201728</td>\n",
       "      <td>0.270414</td>\n",
       "      <td>0.283799</td>\n",
       "      <td>0.343050</td>\n",
       "      <td>0.340233</td>\n",
       "      <td>0.379244</td>\n",
       "      <td>0.378511</td>\n",
       "      <td>0.373017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255070</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.271287</td>\n",
       "      <td>0.328828</td>\n",
       "      <td>0.397950</td>\n",
       "      <td>0.486436</td>\n",
       "      <td>0.530573</td>\n",
       "      <td>0.582752</td>\n",
       "      <td>0.637296</td>\n",
       "      <td>0.637238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>0.199957</td>\n",
       "      <td>0.227188</td>\n",
       "      <td>0.250628</td>\n",
       "      <td>0.265388</td>\n",
       "      <td>0.291736</td>\n",
       "      <td>0.319845</td>\n",
       "      <td>0.339820</td>\n",
       "      <td>0.368420</td>\n",
       "      <td>0.373319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422622</td>\n",
       "      <td>0.410483</td>\n",
       "      <td>0.409814</td>\n",
       "      <td>0.420162</td>\n",
       "      <td>0.426533</td>\n",
       "      <td>0.445706</td>\n",
       "      <td>0.487397</td>\n",
       "      <td>0.495991</td>\n",
       "      <td>0.534095</td>\n",
       "      <td>0.549278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>0.180469</td>\n",
       "      <td>0.142472</td>\n",
       "      <td>0.112432</td>\n",
       "      <td>0.084084</td>\n",
       "      <td>0.071504</td>\n",
       "      <td>0.068979</td>\n",
       "      <td>0.070754</td>\n",
       "      <td>0.063943</td>\n",
       "      <td>0.071106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745875</td>\n",
       "      <td>0.776221</td>\n",
       "      <td>0.767607</td>\n",
       "      <td>0.774457</td>\n",
       "      <td>0.805273</td>\n",
       "      <td>0.802651</td>\n",
       "      <td>0.810866</td>\n",
       "      <td>0.792099</td>\n",
       "      <td>0.796827</td>\n",
       "      <td>0.791949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>0.180695</td>\n",
       "      <td>0.225148</td>\n",
       "      <td>0.220553</td>\n",
       "      <td>0.262136</td>\n",
       "      <td>0.288092</td>\n",
       "      <td>0.280675</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.304840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282820</td>\n",
       "      <td>0.296270</td>\n",
       "      <td>0.324376</td>\n",
       "      <td>0.391588</td>\n",
       "      <td>0.436017</td>\n",
       "      <td>0.500170</td>\n",
       "      <td>0.569207</td>\n",
       "      <td>0.623997</td>\n",
       "      <td>0.673445</td>\n",
       "      <td>0.688012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>0.218762</td>\n",
       "      <td>0.204023</td>\n",
       "      <td>0.207701</td>\n",
       "      <td>0.198991</td>\n",
       "      <td>0.188334</td>\n",
       "      <td>0.173722</td>\n",
       "      <td>0.161461</td>\n",
       "      <td>0.155859</td>\n",
       "      <td>0.136998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650535</td>\n",
       "      <td>0.662007</td>\n",
       "      <td>0.688480</td>\n",
       "      <td>0.708460</td>\n",
       "      <td>0.722464</td>\n",
       "      <td>0.726888</td>\n",
       "      <td>0.758949</td>\n",
       "      <td>0.771153</td>\n",
       "      <td>0.769234</td>\n",
       "      <td>0.785455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>0.383608</td>\n",
       "      <td>0.355178</td>\n",
       "      <td>0.347031</td>\n",
       "      <td>0.338729</td>\n",
       "      <td>0.315327</td>\n",
       "      <td>0.316411</td>\n",
       "      <td>0.318317</td>\n",
       "      <td>0.285892</td>\n",
       "      <td>0.275320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321021</td>\n",
       "      <td>0.335527</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.352514</td>\n",
       "      <td>0.388642</td>\n",
       "      <td>0.390270</td>\n",
       "      <td>0.406929</td>\n",
       "      <td>0.446899</td>\n",
       "      <td>0.451189</td>\n",
       "      <td>0.472153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id         0         1         2         3         4         5  \\\n",
       "0        0  0.535410  0.520775  0.494087  0.465134  0.430339  0.401751   \n",
       "1        1  0.351099  0.398179  0.413809  0.418529  0.433257  0.455410   \n",
       "2        2  0.490537  0.435958  0.413428  0.355796  0.335777  0.299944   \n",
       "3        3  0.051634  0.075802  0.133983  0.154546  0.209387  0.251700   \n",
       "4        4  0.154031  0.201728  0.270414  0.283799  0.343050  0.340233   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  9995  0.199957  0.227188  0.250628  0.265388  0.291736  0.319845   \n",
       "9996  9996  0.180469  0.142472  0.112432  0.084084  0.071504  0.068979   \n",
       "9997  9997  0.169476  0.180695  0.225148  0.220553  0.262136  0.288092   \n",
       "9998  9998  0.218762  0.204023  0.207701  0.198991  0.188334  0.173722   \n",
       "9999  9999  0.383608  0.355178  0.347031  0.338729  0.315327  0.316411   \n",
       "\n",
       "             6         7         8  ...       216       217       218  \\\n",
       "0     0.355986  0.326427  0.282340  ...  0.748339  0.757575  0.768130   \n",
       "1     0.451065  0.464230  0.476011  ...  0.333931  0.276307  0.211513   \n",
       "2     0.242745  0.210555  0.180739  ...  0.709371  0.746826  0.781436   \n",
       "3     0.287552  0.333000  0.340617  ...  0.075046  0.056651  0.079884   \n",
       "4     0.379244  0.378511  0.373017  ...  0.255070  0.242396  0.271287   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.339820  0.368420  0.373319  ...  0.422622  0.410483  0.409814   \n",
       "9996  0.070754  0.063943  0.071106  ...  0.745875  0.776221  0.767607   \n",
       "9997  0.280675  0.312065  0.304840  ...  0.282820  0.296270  0.324376   \n",
       "9998  0.161461  0.155859  0.136998  ...  0.650535  0.662007  0.688480   \n",
       "9999  0.318317  0.285892  0.275320  ...  0.321021  0.335527  0.339900   \n",
       "\n",
       "           219       220       221       222       223       224       225  \n",
       "0     0.777062  0.769173  0.768253  0.738704  0.739460  0.702139  0.702238  \n",
       "1     0.159223  0.110982  0.083130  0.099780  0.145420  0.260501  0.343857  \n",
       "2     0.788292  0.828630  0.835166  0.845859  0.846032  0.836724  0.846779  \n",
       "3     0.147469  0.213112  0.298096  0.382823  0.489381  0.562383  0.599247  \n",
       "4     0.328828  0.397950  0.486436  0.530573  0.582752  0.637296  0.637238  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995  0.420162  0.426533  0.445706  0.487397  0.495991  0.534095  0.549278  \n",
       "9996  0.774457  0.805273  0.802651  0.810866  0.792099  0.796827  0.791949  \n",
       "9997  0.391588  0.436017  0.500170  0.569207  0.623997  0.673445  0.688012  \n",
       "9998  0.708460  0.722464  0.726888  0.758949  0.771153  0.769234  0.785455  \n",
       "9999  0.352514  0.388642  0.390270  0.406929  0.446899  0.451189  0.472153  \n",
       "\n",
       "[10000 rows x 227 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#submission=pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/데이터분석 /bus_ridetime_dacon/12회 대회/submission_제출양식.csv')\n",
    "test=pd.read_csv('\\data/test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oy3OXGDHB7oF"
   },
   "source": [
    "결측치 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y8k1j3MUBcvj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layer_1    0\n",
       "layer_2    0\n",
       "layer_3    0\n",
       "layer_4    0\n",
       "0          0\n",
       "          ..\n",
       "221        0\n",
       "222        0\n",
       "223        0\n",
       "224        0\n",
       "225        0\n",
       "Length: 230, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXC-JzInDihM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['layer_1', 'layer_2', 'layer_3', 'layer_4', '0', '1', '2', '3', '4',\n",
       "       '5',\n",
       "       ...\n",
       "       '216', '217', '218', '219', '220', '221', '222', '223', '224', '225'],\n",
       "      dtype='object', length=230)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "26VTH97rDOdk"
   },
   "outputs": [],
   "source": [
    "# make neural network\n",
    "#!pip install keras\n",
    "from keras import*\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wB60UO-r2NR3"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kbkKfrOXzVLX"
   },
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "learning_rate=0.01\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import recall_score \n",
    "from sklearn.metrics import precision_score \n",
    "from sklearn.metrics import f1_score \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    network=models.Sequential()\n",
    "    network.add(layers.Dense(2000,activation='relu',input_shape=(226,)))\n",
    "    network.add(BatchNormalization())  \n",
    "    network.add(layers.Dense(4000, activation='relu'))\n",
    "    network.add(Dropout(0.05))\n",
    "    network.add(BatchNormalization())  \n",
    "    network.add(layers.Dense(8000, activation='relu'))\n",
    "    network.add(BatchNormalization())  \n",
    "    network.add(Dropout(0.08))\n",
    "    network.add(layers.Dense(7000, activation='relu'))\n",
    "    network.add(BatchNormalization())  \n",
    "    network.add(layers.Dense(3000, activation='relu'))\n",
    "    network.add(Dropout(0.08))\n",
    "    network.add(BatchNormalization())  \n",
    "    network.add(layers.Dense(1000, activation='relu'))\n",
    "    network.add(Dropout(0.08))\n",
    "    network.add(BatchNormalization())  \n",
    "    network.add(layers.Dense(300, activation='relu'))\n",
    "    network.add(BatchNormalization())  \n",
    "    network.add(layers.Dense(4,activation='relu'))\n",
    "\n",
    "    opt=keras.optimizers.Adam(lr=learning_rate,decay=1e-6)\n",
    "    #decay=업데이트 마다 적용되는 학습률의 감소값 \n",
    "\n",
    "    earlystopping = EarlyStopping(monitor='val_loss',  # 모니터 기준 설정 (val loss) \n",
    "                              patience=10,         # 5회 Epoch동안 개선되지 않는다면 종료\n",
    "                             )\n",
    "\n",
    "    network.compile(optimizer=opt,loss='mean_absolute_error',metrics=[precision,recall,f1score])\n",
    "    \n",
    "    layer_4=train.iloc[:,0:4]\n",
    "    layer_else=train.iloc[:,4:]\n",
    "    history=network.fit(layer_else,layer_4, validation_split = 0.3,epochs=1000,batch_size=512,callbacks=[earlystopping])\n",
    "    network.summary()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kEhVIG-78Qg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\solchan bhang\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\Users\\solchan bhang\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 567000 samples, validate on 243000 samples\n",
      "Epoch 1/1000\n",
      "567000/567000 [==============================] - 268s 472us/step - loss: 41.9394 - precision: 1.0000 - recall: 0.9449 - f1score: 0.9651 - val_loss: 20.3583 - val_precision: 1.0000 - val_recall: 0.9941 - val_f1score: 0.9970\n",
      "Epoch 2/1000\n",
      "567000/567000 [==============================] - 273s 481us/step - loss: 11.5523 - precision: 1.0000 - recall: 0.9969 - f1score: 0.9984 - val_loss: 9.7500 - val_precision: 1.0000 - val_recall: 0.9978 - val_f1score: 0.9989\n",
      "Epoch 3/1000\n",
      "567000/567000 [==============================] - 283s 499us/step - loss: 8.3122 - precision: 1.0000 - recall: 0.9989 - f1score: 0.9995 - val_loss: 9.3758 - val_precision: 1.0000 - val_recall: 0.9995 - val_f1score: 0.9997\n",
      "Epoch 4/1000\n",
      "567000/567000 [==============================] - 259s 457us/step - loss: 7.2064 - precision: 1.0000 - recall: 0.9993 - f1score: 0.9996 - val_loss: 7.4262 - val_precision: 1.0000 - val_recall: 0.9993 - val_f1score: 0.9997\n",
      "Epoch 5/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 6.7200 - precision: 1.0000 - recall: 0.9992 - f1score: 0.9996 - val_loss: 6.4200 - val_precision: 1.0000 - val_recall: 0.9997 - val_f1score: 0.9999\n",
      "Epoch 6/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 6.4067 - precision: 1.0000 - recall: 0.9993 - f1score: 0.9996 - val_loss: 6.1618 - val_precision: 1.0000 - val_recall: 0.9996 - val_f1score: 0.9998\n",
      "Epoch 7/1000\n",
      "567000/567000 [==============================] - 258s 456us/step - loss: 6.0455 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 5.0461 - val_precision: 1.0000 - val_recall: 0.9986 - val_f1score: 0.9993\n",
      "Epoch 8/1000\n",
      "567000/567000 [==============================] - 258s 456us/step - loss: 5.7350 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 6.0305 - val_precision: 1.0000 - val_recall: 0.9995 - val_f1score: 0.9998\n",
      "Epoch 9/1000\n",
      "567000/567000 [==============================] - 258s 456us/step - loss: 5.6664 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 6.8718 - val_precision: 1.0000 - val_recall: 0.9998 - val_f1score: 0.9999\n",
      "Epoch 10/1000\n",
      "567000/567000 [==============================] - 258s 456us/step - loss: 5.5511 - precision: 1.0000 - recall: 0.9993 - f1score: 0.9997 - val_loss: 4.3647 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 11/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 5.3892 - precision: 1.0000 - recall: 0.9993 - f1score: 0.9996 - val_loss: 4.4431 - val_precision: 1.0000 - val_recall: 0.9997 - val_f1score: 0.9998\n",
      "Epoch 12/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 5.2402 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9997 - val_loss: 4.5827 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 13/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 5.0815 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 3.7855 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 14/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 5.1100 - precision: 1.0000 - recall: 0.9993 - f1score: 0.9996 - val_loss: 4.6780 - val_precision: 1.0000 - val_recall: 0.9994 - val_f1score: 0.9997\n",
      "Epoch 15/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 5.0481 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 4.1391 - val_precision: 1.0000 - val_recall: 0.9961 - val_f1score: 0.9981\n",
      "Epoch 16/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 4.9391 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 3.1626 - val_precision: 1.0000 - val_recall: 0.9998 - val_f1score: 0.9999\n",
      "Epoch 17/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 4.9462 - precision: 1.0000 - recall: 0.9993 - f1score: 0.9997 - val_loss: 3.7060 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 18/1000\n",
      "567000/567000 [==============================] - 259s 457us/step - loss: 4.8702 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 4.1533 - val_precision: 1.0000 - val_recall: 0.9988 - val_f1score: 0.9994\n",
      "Epoch 19/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 4.6852 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 3.9600 - val_precision: 1.0000 - val_recall: 0.9955 - val_f1score: 0.9977\n",
      "Epoch 20/1000\n",
      "567000/567000 [==============================] - 264s 465us/step - loss: 4.8028 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 3.7059 - val_precision: 1.0000 - val_recall: 0.9998 - val_f1score: 0.9999\n",
      "Epoch 21/1000\n",
      "567000/567000 [==============================] - 279s 492us/step - loss: 4.7056 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 4.2039 - val_precision: 1.0000 - val_recall: 0.9998 - val_f1score: 0.9999\n",
      "Epoch 22/1000\n",
      "567000/567000 [==============================] - 291s 513us/step - loss: 4.6976 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 4.5772 - val_precision: 1.0000 - val_recall: 0.9999 - val_f1score: 1.0000\n",
      "Epoch 23/1000\n",
      "567000/567000 [==============================] - 277s 488us/step - loss: 4.5914 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9997 - val_loss: 2.6874 - val_precision: 1.0000 - val_recall: 0.9999 - val_f1score: 0.9999\n",
      "Epoch 24/1000\n",
      "567000/567000 [==============================] - 284s 502us/step - loss: 4.5479 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9998 - val_loss: 2.8199 - val_precision: 1.0000 - val_recall: 0.9999 - val_f1score: 1.0000\n",
      "Epoch 25/1000\n",
      "567000/567000 [==============================] - 264s 466us/step - loss: 4.5589 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 3.1425 - val_precision: 1.0000 - val_recall: 0.9999 - val_f1score: 0.9999\n",
      "Epoch 26/1000\n",
      "567000/567000 [==============================] - 266s 469us/step - loss: 4.5576 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 3.3354 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 27/1000\n",
      "567000/567000 [==============================] - 266s 469us/step - loss: 4.5307 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 2.9884 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 28/1000\n",
      "567000/567000 [==============================] - 262s 461us/step - loss: 4.4714 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9997 - val_loss: 3.0177 - val_precision: 1.0000 - val_recall: 0.9999 - val_f1score: 0.9999\n",
      "Epoch 29/1000\n",
      "567000/567000 [==============================] - 259s 456us/step - loss: 4.4152 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9997 - val_loss: 2.6656 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 30/1000\n",
      "567000/567000 [==============================] - 263s 464us/step - loss: 4.4114 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 3.1286 - val_precision: 1.0000 - val_recall: 0.9998 - val_f1score: 0.9999\n",
      "Epoch 31/1000\n",
      "567000/567000 [==============================] - 262s 462us/step - loss: 4.4117 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 2.8595 - val_precision: 1.0000 - val_recall: 0.9999 - val_f1score: 1.0000\n",
      "Epoch 32/1000\n",
      "567000/567000 [==============================] - 264s 465us/step - loss: 4.3674 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9997 - val_loss: 2.4530 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 33/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567000/567000 [==============================] - 259s 456us/step - loss: 4.3009 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9997 - val_loss: 2.3649 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 34/1000\n",
      "567000/567000 [==============================] - 261s 461us/step - loss: 4.3480 - precision: 1.0000 - recall: 0.9994 - f1score: 0.9997 - val_loss: 3.3947 - val_precision: 1.0000 - val_recall: 0.9999 - val_f1score: 1.0000\n",
      "Epoch 35/1000\n",
      "567000/567000 [==============================] - 262s 462us/step - loss: 4.2619 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9998 - val_loss: 2.4438 - val_precision: 1.0000 - val_recall: 0.9999 - val_f1score: 1.0000\n",
      "Epoch 36/1000\n",
      "567000/567000 [==============================] - 260s 459us/step - loss: 4.2612 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9997 - val_loss: 3.6120 - val_precision: 1.0000 - val_recall: 1.0000 - val_f1score: 1.0000\n",
      "Epoch 37/1000\n",
      "260608/567000 [============>.................] - ETA: 2:11 - loss: 4.2380 - precision: 1.0000 - recall: 0.9995 - f1score: 0.9997"
     ]
    }
   ],
   "source": [
    "history=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbWChPf16ZS9"
   },
   "outputs": [],
   "source": [
    "#history=network.fit(layer_else,layer_4, validation_split = 0.3,epochs=1000,batch_size=512,callbacks=[earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IFZkq5-klFdf"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.legend(['training_loss', 'validation_loss'], loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7UD8r3LId6p"
   },
   "outputs": [],
   "source": [
    "print(min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjCtxXtLFDrH"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['recall'])\n",
    "plt.plot(history.history['precision'])\n",
    "plt.legend(['recall','precision'], loc = 'upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "반도체박막.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
